# Python 神经网络编程
基本的思路（监督学习）：

- 找到合适的数学模型（比如是线性函数）
- 学习就是使误差更小
- 误差值 E =（ 期望目标 值- 实际输出 值）
- t = (A + Δ A) x
- E= t- y = (A + Δ A) x- Ax
- E = (Δ A) x
- Δ A= L（ E / x）
- L 是学习率，避免过度学习，只能适应某个样本


![](https://d2mxuefqeaa7sj.cloudfront.net/s_9D932484DB6B2DF2E66DDA36537350D333074F6B3BEFE331E23C9B6A13A11737_1534173169858_file.png)


单一的直线无法解决问题的时候，使用多条直线，也就是使用多个分类器，这是神经网络的核心思想


## 反向传播

使用误差 e1 根据权重比例分配到权重链接上
权重 w1,1 = e1 * (w1,1 / (w1,1 + w2,1))
重新分配权重后，再重新进行前向馈送信号的计算，这时使用新的权重

如果有隐藏层，需要先计算出隐藏层的误差，但是隐藏层本来是没有预期输出值的，不过实际上隐藏层误差 e = w1,1 和 w1,2 链接的分割误差之和，为什么是这样，很简单，因为最终的误差也是之前两（多）个权重链接计算后得出来的，因此可以认为这个误差是两（几）者误差之和


![](https://d2mxuefqeaa7sj.cloudfront.net/s_9D932484DB6B2DF2E66DDA36537350D333074F6B3BEFE331E23C9B6A13A11737_1534349927429_file.png)



![](https://d2mxuefqeaa7sj.cloudfront.net/s_9D932484DB6B2DF2E66DDA36537350D333074F6B3BEFE331E23C9B6A13A11737_1534350066535_file.png)



过于复杂的函数可能无法精确的求值，那么可以使用梯度下降（gradient descent）来接近最小值

![](https://d2mxuefqeaa7sj.cloudfront.net/s_9D932484DB6B2DF2E66DDA36537350D333074F6B3BEFE331E23C9B6A13A11737_1534350659474_file.png)


选择多个起始点，避免终止在错误的（山谷）最小值中

